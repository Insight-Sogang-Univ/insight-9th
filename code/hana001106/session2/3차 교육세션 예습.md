# 3차 교육세션 예습 마크다운입니다.
## 읽기
- 데이터분석을 위해서는 데이터를 읽어오는 것이 필수적이다.
- 데이터를 읽어오기 위해서는, 데이터가 어디에 존재하는 지 알아야 하고 그때 사용하는 것이 절대경로와 상대경로이다.

### 절대경로와 상대경로
#### 1. 절대경로
- 절대경로는 처음(파일의 root-뿌리) 시작하여 목적지까지 전체적인 경로를 의미한다.
- 처음부터 경로가 적혀있기 때문에 주로 다른 사람의 문서나 파일에 접근할 때 사용한다.
```python
    https://velog.io/@bami # 웹상의 절대경로 (url)
    C:\Users\insight_hw\session3 # 폴더의 절대경로
```
#### 2. 상대경로
- 상대경로는 현재 위치를 기준으로 목적지까지의 경로를 의미한다.
- 최상위 /를 거치지 않고도 이동 가능하며 내가 어디에 있는 지에 따라 경로는 계속해서 달라진다.
```python
    ./src/components/Counter.js
    ../../Counter.js
```
- 이때 ./등의 기호는 다음과 같은 의미를 가진다
> 최상위 디렉토리 \
    "/" \
> 현재 디렉토리 \
    "./" \
> 현재 디렉토리의 상위 디렉토리 \
    "../"
    
### 데이터를 읽거나 쓰는 다양한 함수

1. CSV (comma separated value)
- Read: read_csv
- Write: to_csv
- 데이터 값을 쉼표로 구분함 

2. MS EXCEL
- Read: read_excel
- Write: to_excel
- 엑셀 상의 행과 열이 데이터프레임의 행과 열로 일대일 대응
- 한번에 여러개 시트 받아올 수 있음 (리스트 형태로)
- 여러개 시트 받아오면 딕셔너리 타입으로 저장됨

3. JSON
- Read: read_json
- Write: to_json

4. HTML
- Read: read_html
- Write: to_html

5. SQL
- Read: read_sql
- Write: to_sql

#### 우리가 자주 사용하게 될 두가지
```python
    pd.read_csv("파일경로.csv")
    pd.read_excel("파일경로.xlsx", sheet_name="시트 이름") # sheet_name은 선택사항
```
- index_col: 불러온 데이터 중 하나의 열을 인덱스로 설정
- usecols: 원하는 열만 불러오기
```python
    pd.read_csv('~~~.csv', index_col = 0)
    pd.read_csv('~~~.csv', usecols=[0,1])
```

## 전처리
- 우리가 가져오게 될 일반적인 데이터에는 누락, 중복등 다양한 오류등이 있는데 이를 수정하고, 변형하는 과정
- 전처리를 통해 데이터의 품질을 올릴 수 있고, 이는 분석의 유용성에 직결되기 때문에 중요하다

- 전처리 순서
1. 데이터 셋 확인
2. 결측값 처리
3. 이상값 처리
4. Feature Engineering

### 결측값(누락데이터) 처리
#### 결측값 확인하기
- info()를 이용하여 요약정보를 보면, NaN 값이 아닌 개수를 보여준다.
- value_counts()를 통해 각 열의 데이터 개수와 NaN 개수 또한 볼 수 있다.
- isnull()을 통해 null인지 아닌지 판단 가능
```python
    df.isnull().sum() # True는 1, False는 0이기 때문에 sum 값이 0이 아닌 값이 나온다면 누락 데이터가 있는 것으로 간주
```
- 가끔가다 누락 데이터가 NaN이 아니라 ? 등의 기호로 표현될 때도 있는데 이때는 replace()를 이용한다.
```python
    df.replace('?', np.nan, inplace="True")
```
#### 결측값 처리하기
1. 삭제: 결측값이 있는 모든 관측지를 삭제하거나 (전체삭제), 모델에 포함시킬 변수들 중에서 삭제 (부분삭제)
    - 전체삭제: 모델의 유효성 감소
    - 부분삭제: 관리 Cost 증가
    ```python
        df.dropna(axis = 1, thresh = 500) # NaN 값이 500개 이상인 열 모두 삭제
    ```
    - 결측값이 무작위로 발생한 경우에 주로 사용한다. 이유가 명확한 결측값인데 삭제를 이용할 경우 모델이 왜곡될 수 있다.
2. 대체: 평균값, 최빈값등을 이용해 null값을 대체
    ```python
        a = df["col1"].mean(axis=0)
        df["col1"].fillna(a, inplace = True) # 평균값으로 대체
        df["col2"].fillna(method = "ffill", inplace = True) # 직전행 값으로 대체
        df["col3"].fillna(method = "bfill", inplace = True) # 직후행 값으로 대체
    ```

### 중복 데이터 처리
- 어떤 행이 같은 값을 가진 경우 중복 데이터, 왜곡이 발생할 수 있기에 확인 후 제거해야함
```python
    df["col1"].duplicated() # 확인
    df["col2"].drop_duplicates() # 제거
    df.drop_duplicates(subsets=["col3", "col4"] # col3,col4를 기준으로 제거
```

### 이상치(outlier) 처리
- 기존 데이터들에서 멀리 떨어져 있는 데이터를 이상치라 한다.
- df.describe(), boxplot등의 시각화, Turkey Fences등을 이용하여 존재유무를 확인한다.

#### 처리방법
1. 전체 삭제
    - Human error에 의해 발생한 경우 삭제한다
2. 다른 값으로 대체
    - 관측치의 양이 적은 경우, 이상치를 제거하기엔 부담이 되므로 평균등으로 대체하거나 예측값으로 대체한다.
3. 변수화
    - 이상치가 자연 발생한 경우, 발생 원인에 대해 분석하고 변수를 추가하여 설명할 수 있다.
4. 리샘플링
    - 이상치를 분리하여 모델을 만든다.

### 범주형 데이터 처리
- 연속형 데이터를 일정한 구간으로 나누어 분석한다.
- 구간분할 binning
- 더미변수: 범주형 -> 연속형, 회귀분석등의 머신러닝 알고리즘에 바로 사용하기 위해 변환

### 정규화 Normalization
- 단위를 변경하거나, 분포가 편향되어 있을 때 사용
- log, square root등의 함수를 이용하여 정규화
- StandardScaler: 각 feature의 평균을 0, 분산을 1로 변경, 모든 특성들이 같은 스케일을 같게 함
```python
    standard_scaler = StandardScaler()
    scaled_data['col1'] = standard_scaler.fit_transform(scaled_data[['col1']])
```
- MinMaxScaler: 모든 feature가 0과 1 사이에 위치하게 만듦, 연산 속도를 높이고 알고리즘 최적화에 good
```python
    scaler = MinMaxScaler(feature_range=(0,1))
```
- RobustScaler: StandardScaler와 유사하지만, 평균과 분산 대신 median과 quartile을 사용하여 이상치에 영향을 받지 않게 함.


















