# 세션 7 예습

## 머신러닝
1. 지도학습: 정답을 알고 있을 때, 분류, 회귀등이 존재한다.
2. 비지도학습: 정답을 모를 때, 군집화, 밀도추정, 차원축소 등
3. 강화학습: 해당 행위에 대한 보상을 통해 학습

## 분류
- 분류란 기존 데이터들을 레이블 단위로 묶고, 그 패턴을 알고리즘에 적용시킨 뒤에 새 데이터도 그 규칙에 따라 어떤 레이블에 들어갈 지 판별하는 것이다.
- 데이터의 차원이 높아졌을 때 아주 유용하다.

## 분류에 쓰이는 대표적인 머신러닝 알고리즘
1. 로지스틱 회귀 (Logistic Regression): 독립변수와 종속변수의 선형 관계성 기반
2. 결정 트리 (Decision Tree): 데이터 균일도에 따른 규칙 기반
3. 서포트 벡터 머신(SVM): 개별 클래스 간의 최대 마진 분류
4. 최소 근접 (Nearest Neighbor): 근접 거리 기준

### 1. Logistic Regression
- 범주형 변수로 사용하는 회귀
- 선형회귀가 그냥 1차 함수처럼 생겼다면,, 얘는 뭔가 원점 대칭처럼 생김
- Odds: 임의의 사건 A가 발생하지 않을 확률 대비 발생할 확률의 비율  
$P(A)/(1-P(A))$
- odds에 log를 씌워 로짓함수 (Logit Function)를 생성한다  
$z = logit(odds) = log(P(A)/(1-P(A)))$
- x는 0 ~ 1, y는 음의 무한대 ~ 양의 무한대
- Logistic function = 로짓 함수의 역함수 (x, y 반대)  
$logistic function = e^\beta*X_i/(1+e^\beta*X_i)$
- 실제 자연현상에는 일반 선형이 아닌 S커브를 따르는 형태가 더 많기 때문에 로지스틱 함수/시그모이드 함수로 표현하게 됨

### 2. Decision Tree
- 직관적으로 이해할 수 있는게 최고 장점
- 논비사 시간에 배웠던 True/False 그래프마냥 생겼다.
- 너무 많이 학습을 시키면 Overfitting이 발생하고, 모든 leaf node가 경우의 수 하나하나를 의미하게 될 수도 있다.

### 3. Support Vector Machine
- 클래스를 분류하는 선을 찾는데, 그 중 가장 적합한 선을 찾는 것
- 이때 이 선을 Margin(마진)이라 한다.
- 선형 비선형 관계 없이 역할 할 수 있음

### 4. kNN
- 데이터들 간의 유사한 특징을 찾아, 그것을 기준으로 그룹을 짓는다.
- 샘플간의 거리 계산을 단순 유클리드 말고 다른 걸로 할 수도 있음
- 장점: 속도가 빠르고 정보 손실이 없다.
- 단점: 쿼리하는 시간이 느림, 용량이 엄청 크다, 이상치에 예민

































